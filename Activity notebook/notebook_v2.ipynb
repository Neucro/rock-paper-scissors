{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building a convnet for ✊✋✌\n",
    "This presents how to build a convnet from scratch to classify images of rock-paper-scissors.  It is meant as a teaching activity to demonstrate the following concepts in practice:\n",
    "- how images are represented and handled in software\n",
    "- how to prepare a machine learning dataset\n",
    "- how a full machine learning pipeline looks\n",
    "- data preprocessing\n",
    "- data augmentation and its importance in a \n",
    "- overfitting, underfitting\n",
    "\n",
    "We use the high-level deep learning library Keras, but the concepts are general and we don't put much focus on the specifics of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 0: defining the problem\n",
    "What problem do we want to solve, exactly?  We want to build a piece of software that, given an image as input that represents an hand making one of the three ✊✋✌ gestures, produces as output a classification of the image in one of the three classes.\n",
    "\n",
    "In the following, we will adopt this convention\n",
    "- class 0 is ✊ rock\n",
    "- class 1 is ✋ paper\n",
    "- class 2 is ✌ scissors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: building a dataset\n",
    "We are starting from scratch, so we need to shoot our own dataset; the best option is that multiple students/groups shoot plenty of images in parallel and then the data is somehow collected.  Whatever the process, in the end we want to have all pics in three different directories, one per class.  Format can be either jpg or png, and landscape/portrait, aspect ratio and resolution don't matter and can be mixed.\n",
    "\n",
    "With some attention to logistics, this can be done in about 10-30 minutes.\n",
    "\n",
    "Guidelines for shooting images. \n",
    "- We don't need high resolution: use the lowest resolution/quality allowed by the phone (this reduces the size of the dataset and speeds up data transfer).\n",
    "- The hand must be more or less in the center of the image; it should not fill the whole image, but it should not be too small either.  ![caption](figures/guidelines.jpg)\n",
    "- we want the dataset to represent as much variability as possible: if we want the classifier to work for all hand orientations, try to have examples for all of them; if we want to handle many different lightling conditions, try to have some pictures for different lightings;\n",
    "- avoid poses that are ambiguous, unless you want to make your job harder: e.g., don't include in the dataset images of paper or scissors taken from the side;\n",
    "- avoid having two images in the dataset that are almost the same: change the camera and hand pose at least a little bit; this is important because in the following code we randomly split training and testing data.\n",
    "\n",
    "Remember that we need the images for each class to be in its own directory. To make this simpler, it helps to shoot first all images of rock, then all images of paper, then all images of scissors, and finally sort the images by time in the file manager and group them accordingly.\n",
    "\n",
    "Place all images in three directories named `c0/`, `c1/`, and `c2/`.  Make sure that each directory only contains image files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: read in images and have a look at them\n",
    "Let's first import what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lale/anaconda3/envs/deep2/lib/python3.5/site-packages/skimage/viewer/utils/core.py:10: UserWarning: Recommended matplotlib backend is `Agg` for full skimage.viewer functionality.\n",
      "  warn(\"Recommended matplotlib backend is `Agg` for full \"\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import skimage\n",
    "import skimage.transform\n",
    "import skimage.viewer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import keras.utils.np_utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup to show interactive jupyter widgets\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "def imgplotList(i,data):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(data[i],interpolation=\"nearest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found directory ../datasets/final/D8/c1 containing class paper\n",
      "Found directory ../datasets/final/D3/c0 containing class rock\n",
      "Found directory ../datasets/final/new/c2 containing class scissors\n",
      "Found directory ../datasets/final/D1/c0 containing class rock\n",
      "Found directory ../datasets/final/D4/c0 containing class rock\n",
      "Found directory ../datasets/final/new/c1 containing class paper\n",
      "Found directory ../datasets/final/D8/c0 containing class rock\n",
      "Found directory ../datasets/final/D1/c1 containing class paper\n",
      "Found directory ../datasets/final/D6/c0 containing class rock\n",
      "Found directory ../datasets/final/D1/c2 containing class scissors\n",
      "Found directory ../datasets/final/D5/c2 containing class scissors\n",
      "Found directory ../datasets/final/D7/c0 containing class rock\n",
      "Found directory ../datasets/final/D5/c0 containing class rock\n",
      "Found directory ../datasets/final/D3/c1 containing class paper\n",
      "Found directory ../datasets/final/testing/c1 containing class paper\n",
      "Found directory ../datasets/final/D6/c2 containing class scissors\n",
      "Found directory ../datasets/final/D7/c2 containing class scissors\n",
      "Found directory ../datasets/final/new/c0 containing class rock\n",
      "Found directory ../datasets/final/testing/c2 containing class scissors\n",
      "Found directory ../datasets/final/D7/c1 containing class paper\n",
      "Found directory ../datasets/final/D2/c1 containing class paper\n",
      "Found directory ../datasets/final/D6/c1 containing class paper\n",
      "Found directory ../datasets/final/D5/c1 containing class paper\n",
      "Found directory ../datasets/final/D2/c0 containing class rock\n",
      "Found directory ../datasets/final/D8/c2 containing class scissors\n",
      "Found directory ../datasets/final/testing/c0 containing class rock\n",
      "Found directory ../datasets/final/D4/c2 containing class scissors\n",
      "Found directory ../datasets/final/D2/c2 containing class scissors\n",
      "Found directory ../datasets/final/D4/c1 containing class paper\n",
      "Found directory ../datasets/final/D3/c2 containing class scissors\n"
     ]
    }
   ],
   "source": [
    "# Define where datasets are located\n",
    "dataset_directory = pathlib.Path(\"..\")/\"datasets\"/\"final\"\n",
    "\n",
    "# Define which datasets we should consider.\n",
    "# Each dataset is a directory withing dataset_directory\n",
    "# and must contain three subdirectories: (c0, c1, c2) for (rock, paper, scissors).\n",
    "dnames = [\"D{}\".format(n) for n in range(1,9)] + [\"testing\"] + [\"new\"]\n",
    "\n",
    "\n",
    "# Now check the data\n",
    "ddirs=[dataset_directory/dn for dn in dnames] # directories of the dataset\n",
    "cdirs={}\n",
    "for ddir in ddirs:\n",
    "    cdirs.update({ddir/\"c0\":0,\n",
    "                  ddir/\"c1\":1,\n",
    "                  ddir/\"c2\":2})\n",
    "names = [\"rock\", \"paper\", \"scissors\"]\n",
    "for cdir,cdir_class in cdirs.items():\n",
    "    assert(cdir.exists())\n",
    "    print(\"Found directory {} containing class {}\".format(cdir,names[cdir_class]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try to read the first image from the first directory, and visualize it.  Note that the tool allows you to zoom in order to see the individual pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = skimage.io.imread(list(list(cdirs.keys())[-1].glob(\"*\"))[0])\n",
    "viewer=skimage.viewer.ImageViewer(im)\n",
    "viewer.show()\n",
    "# Note: you have to close the window to continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now read in all images, cut the central square (with an edge as long as the shorter dimension), and resize it to 200x200 pixels.  Whatever the initial size and orientation of the images, we will end up with a bunch of 200x200 RGB squares in uint8.  These should be small enough that unless the dataset is huge, all should fit in memory.\n",
    "\n",
    "We make a pandas dataframe with the data, with two columns:\n",
    "* image: a $200 \\times 200 \\times 3$ uint8 numpy array\n",
    "* label: on of 0, 1 or 2\n",
    "* file: the full path of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:03<00:53,  1.90s/it]/home/lale/anaconda3/envs/deep2/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: ignoring ../datasets/final/new/c2/._20180530_092919.jpg due to exception cannot identify image file '/home/lale/Desktop/nndemo/rock-paper-scissors/datasets/final/new/c2/._20180530_092919.jpg'\n",
      "  if sys.path[0] == '':\n",
      "/home/lale/anaconda3/envs/deep2/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: ignoring ../datasets/final/new/c2/._20180530_092814.jpg due to exception cannot identify image file '/home/lale/Desktop/nndemo/rock-paper-scissors/datasets/final/new/c2/._20180530_092814.jpg'\n",
      "  if sys.path[0] == '':\n",
      "/home/lale/anaconda3/envs/deep2/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: ignoring ../datasets/final/new/c2/._20180530_092909.jpg due to exception cannot identify image file '/home/lale/Desktop/nndemo/rock-paper-scissors/datasets/final/new/c2/._20180530_092909.jpg'\n",
      "  if sys.path[0] == '':\n",
      " 17%|█▋        | 5/30 [00:05<00:29,  1.17s/it]/home/lale/anaconda3/envs/deep2/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: ignoring ../datasets/final/new/c1/._20180530_092256.jpg due to exception cannot identify image file '/home/lale/Desktop/nndemo/rock-paper-scissors/datasets/final/new/c1/._20180530_092256.jpg'\n",
      "  if sys.path[0] == '':\n",
      "/home/lale/anaconda3/envs/deep2/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: ignoring ../datasets/final/new/c1/._20180530_094114.jpg due to exception cannot identify image file '/home/lale/Desktop/nndemo/rock-paper-scissors/datasets/final/new/c1/._20180530_094114.jpg'\n",
      "  if sys.path[0] == '':\n",
      "/home/lale/anaconda3/envs/deep2/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: ignoring ../datasets/final/new/c1/._20180530_091814.jpg due to exception cannot identify image file '/home/lale/Desktop/nndemo/rock-paper-scissors/datasets/final/new/c1/._20180530_091814.jpg'\n",
      "  if sys.path[0] == '':\n",
      "/home/lale/anaconda3/envs/deep2/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: ignoring ../datasets/final/new/c1/._20180530_090919.jpg due to exception cannot identify image file '/home/lale/Desktop/nndemo/rock-paper-scissors/datasets/final/new/c1/._20180530_090919.jpg'\n",
      "  if sys.path[0] == '':\n",
      " 37%|███▋      | 11/30 [00:12<00:20,  1.10s/it]/home/lale/anaconda3/envs/deep2/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: ignoring ../datasets/final/D7/c0/.DS_Store due to exception cannot identify image file '/home/lale/Desktop/nndemo/rock-paper-scissors/datasets/final/D7/c0/.DS_Store'\n",
      "  if sys.path[0] == '':\n",
      " 53%|█████▎    | 16/30 [00:18<00:15,  1.13s/it]/home/lale/anaconda3/envs/deep2/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: ignoring ../datasets/final/D7/c2/.DS_Store due to exception cannot identify image file '/home/lale/Desktop/nndemo/rock-paper-scissors/datasets/final/D7/c2/.DS_Store'\n",
      "  if sys.path[0] == '':\n",
      " 57%|█████▋    | 17/30 [00:20<00:15,  1.22s/it]/home/lale/anaconda3/envs/deep2/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: ignoring ../datasets/final/new/c0/._20180530_092038.jpg due to exception cannot identify image file '/home/lale/Desktop/nndemo/rock-paper-scissors/datasets/final/new/c0/._20180530_092038.jpg'\n",
      "  if sys.path[0] == '':\n",
      "/home/lale/anaconda3/envs/deep2/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: ignoring ../datasets/final/new/c0/._20180530_091947.jpg due to exception cannot identify image file '/home/lale/Desktop/nndemo/rock-paper-scissors/datasets/final/new/c0/._20180530_091947.jpg'\n",
      "  if sys.path[0] == '':\n",
      "/home/lale/anaconda3/envs/deep2/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: ignoring ../datasets/final/new/c0/._20180530_092343.jpg due to exception cannot identify image file '/home/lale/Desktop/nndemo/rock-paper-scissors/datasets/final/new/c0/._20180530_092343.jpg'\n",
      "  if sys.path[0] == '':\n",
      " 63%|██████▎   | 19/30 [00:21<00:12,  1.12s/it]/home/lale/anaconda3/envs/deep2/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: ignoring ../datasets/final/D7/c1/.DS_Store due to exception cannot identify image file '/home/lale/Desktop/nndemo/rock-paper-scissors/datasets/final/D7/c1/.DS_Store'\n",
      "  if sys.path[0] == '':\n",
      "100%|██████████| 30/30 [00:34<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "imagesize = 200\n",
    "\n",
    "dataset=[]\n",
    "\n",
    "import warnings\n",
    "\n",
    "for cdir,cn in tqdm(list(cdirs.items())):\n",
    "    for f in list(cdir.glob(\"*\")):\n",
    "        try:\n",
    "            im=skimage.io.imread(f)\n",
    "        except (OSError, ValueError) as e:\n",
    "            warnings.warn(\"ignoring {} due to exception {}\".format(f,str(e)))\n",
    "            continue\n",
    "            \n",
    "        h,w=im.shape[0:2] # height, width\n",
    "        sz=min(h,w)\n",
    "        im=im[(h//2-sz//2):(h//2+sz//2),(w//2-sz//2):(w//2+sz//2),:] # defines the central square        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            im=skimage.img_as_ubyte(skimage.transform.resize(im,(imagesize,imagesize))) # resize it to 500x500, whatever the original resolution\n",
    "            \n",
    "        dataset.append({\n",
    "            \"file\":f,\n",
    "            \"label\":cn,\n",
    "            \"image\":im})\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We make a pandas dataframe for the dataset, and create a \"dn\" field containing the name of the dataset from which each image comes (as the name of the directory it was read from)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.DataFrame(dataset)\n",
    "dataset[\"dn\"]=dataset[\"file\"].apply(lambda x: x.parent.parts[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here are 10 random rows from that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style>  \n",
       "<table id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >image</th> \n",
       "        <th class=\"col_heading level0 col1\" >label</th> \n",
       "        <th class=\"col_heading level0 col2\" >file</th> \n",
       "        <th class=\"col_heading level0 col3\" >dn</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67level0_row0\" class=\"row_heading level0 row0\" >157</th> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row0_col0\" class=\"data row0 col0\" >[[[149 145 145]\n",
       "  [150 147 145]\n",
       "  [149 144 147]\n",
       "  ..., \n",
       "  [144 149 145]\n",
       "  [140 143 136]\n",
       "  [147 149 144]]\n",
       "\n",
       " [[148 147 143]\n",
       "  [151 150 147]\n",
       "  [147 145 150]\n",
       "  ..., \n",
       "  [129 134 130]\n",
       "  [136 140 135]\n",
       "  [152 159 152]]\n",
       "\n",
       " [[149 149 146]\n",
       "  [152 148 148]\n",
       "  [147 149 146]\n",
       "  ..., \n",
       "  [108 111 108]\n",
       "  [132 138 130]\n",
       "  [148 156 146]]\n",
       "\n",
       " ..., \n",
       " [[114 118 113]\n",
       "  [116 116 112]\n",
       "  [112 116 114]\n",
       "  ..., \n",
       "  [121 112 104]\n",
       "  [124 114 107]\n",
       "  [125 115 104]]\n",
       "\n",
       " [[120 121 119]\n",
       "  [123 125 120]\n",
       "  [124 125 124]\n",
       "  ..., \n",
       "  [121 109 104]\n",
       "  [122 109 101]\n",
       "  [122 114 106]]\n",
       "\n",
       " [[115 115 114]\n",
       "  [117 118 115]\n",
       "  [115 118 116]\n",
       "  ..., \n",
       "  [113 104  95]\n",
       "  [120 107  97]\n",
       "  [118 105  99]]]</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row0_col1\" class=\"data row0 col1\" >1</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row0_col2\" class=\"data row0 col2\" >../datasets/final/D8/c1/20180327143327-Paper-fd3b563d3a42d605.jpg</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row0_col3\" class=\"data row0 col3\" >D8</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67level0_row1\" class=\"row_heading level0 row1\" >1919</th> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row1_col0\" class=\"data row1 col0\" >[[[252 252 252]\n",
       "  [252 252 252]\n",
       "  [252 252 252]\n",
       "  ..., \n",
       "  [137 146 154]\n",
       "  [135 142 151]\n",
       "  [134 141 149]]\n",
       "\n",
       " [[253 253 253]\n",
       "  [253 253 253]\n",
       "  [253 253 253]\n",
       "  ..., \n",
       "  [139 146 155]\n",
       "  [138 145 153]\n",
       "  [137 144 152]]\n",
       "\n",
       " [[251 253 252]\n",
       "  [252 254 253]\n",
       "  [253 254 253]\n",
       "  ..., \n",
       "  [141 145 155]\n",
       "  [139 145 154]\n",
       "  [136 143 151]]\n",
       "\n",
       " ..., \n",
       " [[182 153 156]\n",
       "  [183 152 160]\n",
       "  [174 152 156]\n",
       "  ..., \n",
       "  [ 48  42  42]\n",
       "  [ 39  34  31]\n",
       "  [ 30  27  22]]\n",
       "\n",
       " [[181 148 153]\n",
       "  [173 145 151]\n",
       "  [170 141 147]\n",
       "  ..., \n",
       "  [ 43  40  37]\n",
       "  [ 41  37  32]\n",
       "  [ 29  26  20]]\n",
       "\n",
       " [[176 143 150]\n",
       "  [167 140 147]\n",
       "  [166 135 142]\n",
       "  ..., \n",
       "  [ 39  37  33]\n",
       "  [ 42  39  34]\n",
       "  [ 32  29  24]]]</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row1_col1\" class=\"data row1 col1\" >1</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row1_col2\" class=\"data row1 col2\" >../datasets/final/D5/c1/20140716_231423.jpg</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row1_col3\" class=\"data row1 col3\" >D5</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67level0_row2\" class=\"row_heading level0 row2\" >933</th> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row2_col0\" class=\"data row2 col0\" >[[[ 30  28  46]\n",
       "  [ 33  32  53]\n",
       "  [ 31  32  53]\n",
       "  ..., \n",
       "  [102  78  71]\n",
       "  [100  77  70]\n",
       "  [105  82  75]]\n",
       "\n",
       " [[ 33  31  52]\n",
       "  [ 37  36  60]\n",
       "  [ 34  35  57]\n",
       "  ..., \n",
       "  [102  79  71]\n",
       "  [105  83  72]\n",
       "  [110  87  79]]\n",
       "\n",
       " [[ 31  30  57]\n",
       "  [ 28  32  56]\n",
       "  [ 30  34  59]\n",
       "  ..., \n",
       "  [108  85  79]\n",
       "  [107  85  75]\n",
       "  [111  86  79]]\n",
       "\n",
       " ..., \n",
       " [[206 206 202]\n",
       "  [206 203 199]\n",
       "  [205 200 199]\n",
       "  ..., \n",
       "  [143 158 176]\n",
       "  [140 155 172]\n",
       "  [143 156 173]]\n",
       "\n",
       " [[219 227 235]\n",
       "  [219 223 230]\n",
       "  [217 221 228]\n",
       "  ..., \n",
       "  [137 154 171]\n",
       "  [139 154 173]\n",
       "  [142 155 173]]\n",
       "\n",
       " [[222 230 241]\n",
       "  [219 227 238]\n",
       "  [219 228 237]\n",
       "  ..., \n",
       "  [139 152 171]\n",
       "  [137 151 170]\n",
       "  [138 152 171]]]</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row2_col1\" class=\"data row2 col1\" >0</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row2_col2\" class=\"data row2 col2\" >../datasets/final/D7/c0/20180219105755-Rock-f823dda185d6c9f2.jpg</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row2_col3\" class=\"data row2 col3\" >D7</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67level0_row3\" class=\"row_heading level0 row3\" >2379</th> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row3_col0\" class=\"data row3 col0\" >[[[194 186 175]\n",
       "  [192 184 173]\n",
       "  [194 186 175]\n",
       "  ..., \n",
       "  [149 154 150]\n",
       "  [170 174 174]\n",
       "  [171 175 176]]\n",
       "\n",
       " [[193 185 174]\n",
       "  [195 187 176]\n",
       "  [194 186 175]\n",
       "  ..., \n",
       "  [163 168 164]\n",
       "  [171 175 175]\n",
       "  [170 174 175]]\n",
       "\n",
       " [[194 186 175]\n",
       "  [193 185 174]\n",
       "  [193 185 174]\n",
       "  ..., \n",
       "  [172 177 174]\n",
       "  [172 176 176]\n",
       "  [171 175 176]]\n",
       "\n",
       " ..., \n",
       " [[120  70  55]\n",
       "  [121  73  59]\n",
       "  [119  78  66]\n",
       "  ..., \n",
       "  [189 184 180]\n",
       "  [188 183 179]\n",
       "  [188 183 179]]\n",
       "\n",
       " [[120  69  56]\n",
       "  [125  72  58]\n",
       "  [130  79  65]\n",
       "  ..., \n",
       "  [190 183 180]\n",
       "  [189 183 179]\n",
       "  [189 183 179]]\n",
       "\n",
       " [[126  74  61]\n",
       "  [133  79  65]\n",
       "  [136  82  68]\n",
       "  ..., \n",
       "  [189 181 178]\n",
       "  [187 181 177]\n",
       "  [186 181 177]]]</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row3_col1\" class=\"data row3 col1\" >1</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row3_col2\" class=\"data row3 col2\" >../datasets/final/D4/c1/IMG_20170802_160219.jpg</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row3_col3\" class=\"data row3 col3\" >D4</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67level0_row4\" class=\"row_heading level0 row4\" >2076</th> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row4_col0\" class=\"data row4 col0\" >[[[143 139 137]\n",
       "  [155 154 152]\n",
       "  [154 154 154]\n",
       "  ..., \n",
       "  [ 18  25  20]\n",
       "  [ 26  33  24]\n",
       "  [ 50  56  46]]\n",
       "\n",
       " [[136 135 132]\n",
       "  [160 160 158]\n",
       "  [158 159 157]\n",
       "  ..., \n",
       "  [ 18  21  15]\n",
       "  [ 26  33  22]\n",
       "  [ 40  47  33]]\n",
       "\n",
       " [[141 139 136]\n",
       "  [157 158 158]\n",
       "  [166 163 164]\n",
       "  ..., \n",
       "  [ 19  20  19]\n",
       "  [ 20  24  17]\n",
       "  [ 31  34  24]]\n",
       "\n",
       " ..., \n",
       " [[160 163 162]\n",
       "  [176 176 180]\n",
       "  [151 153 153]\n",
       "  ..., \n",
       "  [109 101  84]\n",
       "  [ 90  74  49]\n",
       "  [ 85  65  45]]\n",
       "\n",
       " [[144 145 145]\n",
       "  [170 170 171]\n",
       "  [181 180 179]\n",
       "  ..., \n",
       "  [133 133 128]\n",
       "  [108 102  91]\n",
       "  [ 90  78  56]]\n",
       "\n",
       " [[146 149 147]\n",
       "  [151 152 152]\n",
       "  [173 171 172]\n",
       "  ..., \n",
       "  [142 146 141]\n",
       "  [141 143 139]\n",
       "  [116 117 108]]]</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row4_col1\" class=\"data row4 col1\" >2</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row4_col2\" class=\"data row4 col2\" >../datasets/final/D8/c2/20180327143144-Scissors-fd3b563d3a42d605.jpg</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row4_col3\" class=\"data row4 col3\" >D8</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67level0_row5\" class=\"row_heading level0 row5\" >2225</th> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row5_col0\" class=\"data row5 col0\" >[[[189 133  75]\n",
       "  [187 131  72]\n",
       "  [189 131  71]\n",
       "  ..., \n",
       "  [194 136  73]\n",
       "  [195 138  77]\n",
       "  [196 140  79]]\n",
       "\n",
       " [[189 133  74]\n",
       "  [190 134  75]\n",
       "  [192 134  74]\n",
       "  ..., \n",
       "  [199 141  78]\n",
       "  [197 138  77]\n",
       "  [197 138  78]]\n",
       "\n",
       " [[189 133  74]\n",
       "  [190 134  75]\n",
       "  [192 133  73]\n",
       "  ..., \n",
       "  [198 140  76]\n",
       "  [197 138  75]\n",
       "  [201 141  79]]\n",
       "\n",
       " ..., \n",
       " [[ 90  87  80]\n",
       "  [ 88  85  77]\n",
       "  [ 85  82  73]\n",
       "  ..., \n",
       "  [ 46  49  56]\n",
       "  [ 52  51  59]\n",
       "  [ 55  54  62]]\n",
       "\n",
       " [[ 84  81  72]\n",
       "  [ 80  76  67]\n",
       "  [ 70  64  56]\n",
       "  ..., \n",
       "  [ 72  75  82]\n",
       "  [ 62  63  71]\n",
       "  [ 44  43  51]]\n",
       "\n",
       " [[ 71  67  58]\n",
       "  [ 59  55  46]\n",
       "  [ 61  55  47]\n",
       "  ..., \n",
       "  [ 43  46  53]\n",
       "  [ 60  63  70]\n",
       "  [ 73  73  81]]]</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row5_col1\" class=\"data row5 col1\" >2</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row5_col2\" class=\"data row5 col2\" >../datasets/final/D4/c2/IMG_20170802_160310.jpg</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row5_col3\" class=\"data row5 col3\" >D4</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67level0_row6\" class=\"row_heading level0 row6\" >1274</th> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row6_col0\" class=\"data row6 col0\" >[[[117  95  72]\n",
       "  [116  94  71]\n",
       "  [102  80  56]\n",
       "  ..., \n",
       "  [135 128 112]\n",
       "  [118 109  98]\n",
       "  [135 125 116]]\n",
       "\n",
       " [[126 104  81]\n",
       "  [107  85  62]\n",
       "  [117  95  71]\n",
       "  ..., \n",
       "  [156 147 133]\n",
       "  [131 119 106]\n",
       "  [136 124 112]]\n",
       "\n",
       " [[112  90  67]\n",
       "  [120  98  75]\n",
       "  [123 101  78]\n",
       "  ..., \n",
       "  [132 120 108]\n",
       "  [147 134 121]\n",
       "  [143 130 116]]\n",
       "\n",
       " ..., \n",
       " [[ 35  31  45]\n",
       "  [ 32  30  43]\n",
       "  [ 31  30  43]\n",
       "  ..., \n",
       "  [ 76  55  34]\n",
       "  [ 80  61  39]\n",
       "  [ 86  68  46]]\n",
       "\n",
       " [[ 32  33  48]\n",
       "  [ 24  25  40]\n",
       "  [ 31  32  45]\n",
       "  ..., \n",
       "  [ 80  63  41]\n",
       "  [ 84  65  43]\n",
       "  [ 77  57  36]]\n",
       "\n",
       " [[ 25  27  42]\n",
       "  [ 26  28  43]\n",
       "  [ 24  26  39]\n",
       "  ..., \n",
       "  [ 66  50  27]\n",
       "  [ 64  44  23]\n",
       "  [ 83  62  41]]]</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row6_col1\" class=\"data row6 col1\" >2</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row6_col2\" class=\"data row6 col2\" >../datasets/final/D6/c2/IMG_20180119_100534.jpg</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row6_col3\" class=\"data row6 col3\" >D6</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67level0_row7\" class=\"row_heading level0 row7\" >529</th> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row7_col0\" class=\"data row7 col0\" >[[[158 159 151]\n",
       "  [161 164 156]\n",
       "  [163 166 154]\n",
       "  ..., \n",
       "  [  7  21  32]\n",
       "  [  5  24  35]\n",
       "  [ 11  27  43]]\n",
       "\n",
       " [[160 164 154]\n",
       "  [161 162 152]\n",
       "  [160 161 149]\n",
       "  ..., \n",
       "  [  5  18  29]\n",
       "  [  5  20  32]\n",
       "  [  9  22  35]]\n",
       "\n",
       " [[152 155 146]\n",
       "  [163 164 152]\n",
       "  [169 168 156]\n",
       "  ..., \n",
       "  [  6  18  29]\n",
       "  [  7  18  28]\n",
       "  [ 11  18  28]]\n",
       "\n",
       " ..., \n",
       " [[183 193 197]\n",
       "  [104 104 102]\n",
       "  [132 134 134]\n",
       "  ..., \n",
       "  [ 76  77  65]\n",
       "  [ 73  72  64]\n",
       "  [ 70  71  60]]\n",
       "\n",
       " [[108 112 115]\n",
       "  [123 124 122]\n",
       "  [139 141 140]\n",
       "  ..., \n",
       "  [ 75  74  63]\n",
       "  [ 74  74  65]\n",
       "  [ 70  71  61]]\n",
       "\n",
       " [[103 107 103]\n",
       "  [134 134 134]\n",
       "  [141 146 147]\n",
       "  ..., \n",
       "  [ 75  77  65]\n",
       "  [ 76  75  66]\n",
       "  [ 71  72  62]]]</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row7_col1\" class=\"data row7 col1\" >0</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row7_col2\" class=\"data row7 col2\" >../datasets/final/D8/c0/20180327130625-Rock-fd3b563d3a42d605.jpg</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row7_col3\" class=\"data row7 col3\" >D8</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67level0_row8\" class=\"row_heading level0 row8\" >334</th> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row8_col0\" class=\"data row8 col0\" >[[[  3   1   4]\n",
       "  [  3   1   5]\n",
       "  [  4   3   8]\n",
       "  ..., \n",
       "  [111 107 106]\n",
       "  [108 106 103]\n",
       "  [106 105 101]]\n",
       "\n",
       " [[  2   3   5]\n",
       "  [  4   3   8]\n",
       "  [  4   3   8]\n",
       "  ..., \n",
       "  [107 106 104]\n",
       "  [108 107 104]\n",
       "  [108 107 103]]\n",
       "\n",
       " [[  2   7  12]\n",
       "  [  1   4  10]\n",
       "  [  1   4  10]\n",
       "  ..., \n",
       "  [109 108 104]\n",
       "  [109 108 105]\n",
       "  [108 107 105]]\n",
       "\n",
       " ..., \n",
       " [[175 179 190]\n",
       "  [175 179 188]\n",
       "  [178 181 190]\n",
       "  ..., \n",
       "  [132 130 131]\n",
       "  [132 131 130]\n",
       "  [130 129 127]]\n",
       "\n",
       " [[174 180 191]\n",
       "  [176 181 192]\n",
       "  [175 181 190]\n",
       "  ..., \n",
       "  [131 130 130]\n",
       "  [131 130 131]\n",
       "  [130 128 130]]\n",
       "\n",
       " [[172 178 190]\n",
       "  [174 181 192]\n",
       "  [173 180 190]\n",
       "  ..., \n",
       "  [131 130 130]\n",
       "  [130 128 130]\n",
       "  [130 128 131]]]</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row8_col1\" class=\"data row8 col1\" >0</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row8_col2\" class=\"data row8 col2\" >../datasets/final/D4/c0/20170802_162744.jpg</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row8_col3\" class=\"data row8 col3\" >D4</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67level0_row9\" class=\"row_heading level0 row9\" >1386</th> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row9_col0\" class=\"data row9 col0\" >[[[159 133 120]\n",
       "  [162 143 133]\n",
       "  [160 145 138]\n",
       "  ..., \n",
       "  [176 165 159]\n",
       "  [166 157 150]\n",
       "  [169 160 153]]\n",
       "\n",
       " [[162 137 127]\n",
       "  [161 141 134]\n",
       "  [161 142 136]\n",
       "  ..., \n",
       "  [173 159 151]\n",
       "  [177 166 159]\n",
       "  [162 155 148]]\n",
       "\n",
       " [[164 140 133]\n",
       "  [167 142 135]\n",
       "  [159 137 129]\n",
       "  ..., \n",
       "  [170 150 143]\n",
       "  [171 154 146]\n",
       "  [177 165 158]]\n",
       "\n",
       " ..., \n",
       " [[186 172 156]\n",
       "  [183 175 156]\n",
       "  [186 174 156]\n",
       "  ..., \n",
       "  [198 185 171]\n",
       "  [199 185 173]\n",
       "  [197 185 168]]\n",
       "\n",
       " [[185 173 156]\n",
       "  [184 173 155]\n",
       "  [185 174 155]\n",
       "  ..., \n",
       "  [196 182 170]\n",
       "  [197 184 170]\n",
       "  [196 182 169]]\n",
       "\n",
       " [[184 173 155]\n",
       "  [183 172 155]\n",
       "  [182 173 153]\n",
       "  ..., \n",
       "  [196 183 167]\n",
       "  [196 182 169]\n",
       "  [196 182 169]]]</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row9_col1\" class=\"data row9 col1\" >2</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row9_col2\" class=\"data row9 col2\" >../datasets/final/D7/c2/20180219134335-Scissors-d10227e15e2074d5.jpg</td> \n",
       "        <td id=\"T_e0d0ccda_6e49_11e8_aff0_02427d6cdf67row9_col3\" class=\"data row9 col3\" >D7</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb56419db70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sample(n=10)[[\"image\",\"label\",\"file\",\"dn\"]].style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Optionally, we can quickly scroll through the images in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer=skimage.viewer.CollectionViewer([r[1][\"image\"] for r in dataset.sort_values(\"dn\").iterrows()])\n",
    "viewer.show()\n",
    "# Note: you have to close the window to continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb564132c18>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb564781b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "im = dataset.loc[0,\"image\"]\n",
    "sns.heatmap(skimage.color.rgb2gray(im[50:52,50:52]),cmap=\"gray\",annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Prepare training and testing sets\n",
    "\n",
    "How should we split training and testing data?  The code below implements a few options (run only one of the cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Option one (hard): use all samples from dataset named \"testing\"\n",
    "# which contains some images taken in the same days as D1--D4, but not contained in these dirs.\n",
    "te_mask = ((dataset[\"dn\"]==\"testing\") | (dataset[\"dn\"]==\"new\"))\n",
    "dataset_te=dataset[te_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option two (hard): test on all samples from one specific dataset\n",
    "te_mask = dataset[\"dn\"]==\"D2\"\n",
    "dataset_te=dataset[te_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Training images</th>\n",
       "      <th># Testing images</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rock</th>\n",
       "      <td>784</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper</th>\n",
       "      <td>757</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scissors</th>\n",
       "      <td>798</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            # Training images  # Testing images\n",
       "Class name                                     \n",
       "rock                      784                34\n",
       "paper                     757                35\n",
       "scissors                  798                34"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In any case, training samples are all other samples\n",
    "dataset_tr=dataset.loc[dataset.index.difference(dataset_te.index)]\n",
    "\n",
    "# Print a summary of how many training and testing images we have sampled\n",
    "import collections\n",
    "pd.DataFrame(index=[0,1,2],data=collections.OrderedDict((\n",
    "    (\"Class name\",           names),\n",
    "    (\"# Training images\", dataset_tr[\"label\"].value_counts()),\n",
    "    (\"# Testing images\",  dataset_te[\"label\"].value_counts())))).set_index(\"Class name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30b717bd13a4332bc4e2c01b25e12cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='i', max=102), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.imgplotList>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "examples=list(dataset_te[\"image\"])\n",
    "interact(\n",
    "    imgplotList, \n",
    "    i=widgets.IntSlider(min=0,max=len(examples)-1, step=1, value=0,continuous_update=True), \n",
    "    data=fixed(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: define what we feed to the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200, 3) uint8\n"
     ]
    }
   ],
   "source": [
    "im = dataset_tr.sample(1).iloc[0][\"image\"]\n",
    "print(im.shape, im.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Take image and resize to a specified size\n",
    "def transform_simple(im,sz):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        imr = skimage.transform.resize(im, (sz,sz))\n",
    "    return imr\n",
    "\n",
    "transform = transform_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Take image and resize to a specified size, after applying data augmentation\n",
    "def transform_complex(im,sz):\n",
    "    if(np.random.rand()<0.5):\n",
    "        im=np.fliplr(im)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        tf1 = skimage.transform.SimilarityTransform(scale = 1 / im.shape[0])\n",
    "        tf2 = skimage.transform.SimilarityTransform(translation=[-0.5, -0.5])\n",
    "        tf3 = skimage.transform.SimilarityTransform(rotation=np.deg2rad(np.random.uniform(0,360)))\n",
    "        tf4 = skimage.transform.SimilarityTransform(scale=np.random.uniform(1,1.6))\n",
    "        tf5 = skimage.transform.SimilarityTransform(translation=np.array([0.5, 0.5])+np.random.uniform(-0.1,0.1,size=2))\n",
    "        tf6 = skimage.transform.SimilarityTransform(scale=sz)\n",
    "        imr = skimage.transform.warp(im, (tf1+(tf2+(tf3+(tf4+(tf5+tf6))))).inverse, output_shape=(sz,sz),mode=\"edge\")\n",
    "        imr = imr*np.random.uniform(0.9,1.1,size=(1,1,3))\n",
    "        imr = np.clip(imr,0,1)\n",
    "    return imr\n",
    "\n",
    "transform = transform_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6c573d2f4849f0af07789a33deb172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='sz', min=2), IntSlider(value=0, description='seed'), ToggleButton(value=False, description='Reveal'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The resolution challenge\n",
    "@interact(sz = widgets.IntSlider(min=2,max=100),\n",
    "          seed = widgets.IntSlider(min=0,max=100),\n",
    "          reveal = widgets.widgets.ToggleButton(value=False,description='Reveal'))\n",
    "def f(sz,seed,reveal):\n",
    "    fig,axs = plt.subplots(nrows = 2, ncols = 5,figsize=(6,3),dpi=150)\n",
    "    ims = dataset_tr.sample(len(axs.flatten()), random_state=sz*100+seed)\n",
    "    for ax,(_,row) in zip(axs.flatten(),ims.iterrows()):\n",
    "        ax.imshow(transform_simple(row[\"image\"],sz))\n",
    "        ax.axis(\"off\")\n",
    "        if(reveal):\n",
    "            ax.set_title(names[row[\"label\"]])\n",
    "    #fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def sample(df,sz):\n",
    "    r=df.sample(n=1)\n",
    "    l=r[\"label\"].iloc[0]\n",
    "    im=r[\"image\"].iloc[0]\n",
    "    im=transform(im,sz)\n",
    "    return im,l\n",
    "\n",
    "def mkbatch(df,N,sz):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(N):\n",
    "        im,l=sample(df,sz)\n",
    "        X.append(im)\n",
    "        y.append(l)\n",
    "    X=np.array(X).astype('float32')\n",
    "    y=np.array(y)\n",
    "    y=keras.utils.np_utils.to_categorical(y,3)\n",
    "    return X,y\n",
    "\n",
    "def generator(df,batch_size,sz):\n",
    "    while True:\n",
    "        X,y = mkbatch(df,batch_size,sz)\n",
    "        yield (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "b = mkbatch(dataset_tr,20,32)\n",
    "b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69f90a6dcae4da6a2c212f43fbd493f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='i', max=99), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.imgplotList>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize 100 images from our input dataset\n",
    "examples = list(mkbatch(dataset_tr,100,32)[0])\n",
    "interact(imgplotList, i=widgets.IntSlider(min=0, max=len(examples)-1, step=1, value=0,continuous_update=False), data=fixed(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916f0493dbfe411ab6d78fc71748449b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='i', max=99), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.imgplotList>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize 100 variations from our first input image (makes sense only if transform==transform_complex, i.e. if we are using data augmentation)\n",
    "examples = list(mkbatch(dataset_tr.iloc[[0]],100,32)[0])\n",
    "interact(imgplotList, i=widgets.IntSlider(min=0, max=len(examples)-1, step=1, value=0,continuous_update=False), data=fixed(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 5: build and train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras: Deep Learning library for Theano and TensorFlow\n",
    "import keras\n",
    "from keras.utils  import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dropout\n",
    "\n",
    "# NN settings\n",
    "patchsize          = 32\n",
    "batch_size         = 32\n",
    "pool_size          = (2,2) # size of pooling area for max pooling\n",
    "kernel_size        = (3,3) # convolution kernel size\n",
    "\n",
    "def makeModel(nb_filters):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(nb_filters, kernel_size, input_shape=(patchsize,patchsize,3), padding = \"same\"))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = pool_size))\n",
    "    model.add(Conv2D(nb_filters*2, kernel_size, padding = \"same\"))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = pool_size))\n",
    "    model.add(Conv2D(nb_filters*4, kernel_size, padding = \"same\"))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = pool_size))\n",
    "    model.add(AveragePooling2D(pool_size = pool_size))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128)) # generate a fully connected layer wiht 128 outputs (arbitrary value)\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3)) # output layer\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    ## compile! network\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeModel(128).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Build a set of 1000 testing instances taken from the testing dataset.\n",
    "\n",
    "Note: \"testing\" in this case is synonym with \"validation\" and \"evaluation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_test,y_test) = mkbatch(dataset_te, 1000, patchsize)\n",
    "\n",
    "# Prepare the logs directory, if it does not exist\n",
    "(pathlib.Path(\".\")/\"logs\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While the network trains, we can monitor training loss/accuracy and testing loss/accuracy using tensorboard at http://0.0.0.0:6006\n",
    "\n",
    "You may need to launch tensorboard first if it's not already running, by executing\n",
    "\n",
    "`tensorboard --logdir=logs`\n",
    "\n",
    "in a shell with the current working directory.  Check that you are within the proper conda environment, if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 1.1053 - acc: 0.3050 - val_loss: 1.0985 - val_acc: 0.3410\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 4s 82ms/step - loss: 1.0975 - acc: 0.3569 - val_loss: 1.0995 - val_acc: 0.3410\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 4s 83ms/step - loss: 1.0974 - acc: 0.3506 - val_loss: 1.0996 - val_acc: 0.3410\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 1.0988 - acc: 0.3475 - val_loss: 1.0987 - val_acc: 0.3290\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 4s 86ms/step - loss: 1.0978 - acc: 0.3463 - val_loss: 1.0959 - val_acc: 0.3920\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 4s 84ms/step - loss: 1.0972 - acc: 0.3531 - val_loss: 1.0966 - val_acc: 0.3810\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 1.0934 - acc: 0.3700 - val_loss: 1.0931 - val_acc: 0.3470\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 4s 86ms/step - loss: 1.0910 - acc: 0.3831 - val_loss: 1.0753 - val_acc: 0.4650\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 1.0622 - acc: 0.4188 - val_loss: 1.0303 - val_acc: 0.4580\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 1.0527 - acc: 0.4575 - val_loss: 1.0151 - val_acc: 0.5140\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 4s 84ms/step - loss: 1.0236 - acc: 0.4725 - val_loss: 0.9666 - val_acc: 0.5610\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 4s 84ms/step - loss: 0.9921 - acc: 0.5106 - val_loss: 0.9514 - val_acc: 0.4880\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 4s 84ms/step - loss: 0.9276 - acc: 0.5613 - val_loss: 0.9022 - val_acc: 0.5750\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 0.9109 - acc: 0.5706 - val_loss: 0.8977 - val_acc: 0.6230\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 4s 84ms/step - loss: 0.8162 - acc: 0.6488 - val_loss: 0.8342 - val_acc: 0.6170\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 4s 83ms/step - loss: 0.7842 - acc: 0.6787 - val_loss: 0.8381 - val_acc: 0.6050\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 4s 88ms/step - loss: 0.7221 - acc: 0.7044 - val_loss: 0.8521 - val_acc: 0.6140\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 0.6896 - acc: 0.7206 - val_loss: 0.8284 - val_acc: 0.6210\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 0.6662 - acc: 0.7475 - val_loss: 0.8176 - val_acc: 0.6430\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 4s 84ms/step - loss: 0.5962 - acc: 0.7725 - val_loss: 0.8656 - val_acc: 0.6220\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 4s 84ms/step - loss: 0.5603 - acc: 0.7644 - val_loss: 0.7454 - val_acc: 0.6430\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 4s 86ms/step - loss: 0.4953 - acc: 0.8100 - val_loss: 0.8847 - val_acc: 0.6300\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.4775 - acc: 0.8162 - val_loss: 0.7580 - val_acc: 0.6740\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.4296 - acc: 0.8419 - val_loss: 0.7798 - val_acc: 0.6730\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 0.4076 - acc: 0.8438 - val_loss: 0.8474 - val_acc: 0.6370\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 4s 90ms/step - loss: 0.3539 - acc: 0.8731 - val_loss: 0.7747 - val_acc: 0.7030\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 0.3246 - acc: 0.8781 - val_loss: 0.9861 - val_acc: 0.6220\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 0.2965 - acc: 0.8994 - val_loss: 1.0270 - val_acc: 0.6350\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 0.2644 - acc: 0.9044 - val_loss: 0.8025 - val_acc: 0.6780\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 4s 86ms/step - loss: 0.2196 - acc: 0.9306 - val_loss: 0.8536 - val_acc: 0.6500\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 0.2076 - acc: 0.9263 - val_loss: 0.9093 - val_acc: 0.6670\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 5s 95ms/step - loss: 0.2273 - acc: 0.9269 - val_loss: 1.0388 - val_acc: 0.6390\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 0.1678 - acc: 0.9431 - val_loss: 0.9177 - val_acc: 0.6710\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 0.1531 - acc: 0.9525 - val_loss: 0.8420 - val_acc: 0.7240\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 0.1307 - acc: 0.9581 - val_loss: 1.1097 - val_acc: 0.5890\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 0.1216 - acc: 0.9644 - val_loss: 0.8485 - val_acc: 0.7010\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 4s 74ms/step - loss: 0.1199 - acc: 0.9587 - val_loss: 1.2430 - val_acc: 0.6930\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 4s 73ms/step - loss: 0.0920 - acc: 0.9706 - val_loss: 1.1214 - val_acc: 0.6260\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 4s 73ms/step - loss: 0.0751 - acc: 0.9825 - val_loss: 1.2128 - val_acc: 0.6250\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 4s 73ms/step - loss: 0.0783 - acc: 0.9800 - val_loss: 0.9240 - val_acc: 0.6950\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 0.0582 - acc: 0.9838 - val_loss: 0.8420 - val_acc: 0.7330\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.0598 - acc: 0.9831 - val_loss: 0.9971 - val_acc: 0.6990\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 4s 88ms/step - loss: 0.0425 - acc: 0.9894 - val_loss: 1.0875 - val_acc: 0.6960\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 4s 86ms/step - loss: 0.0569 - acc: 0.9856 - val_loss: 0.9338 - val_acc: 0.6670\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 0.0413 - acc: 0.9894 - val_loss: 1.3378 - val_acc: 0.6920\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 4s 86ms/step - loss: 0.0328 - acc: 0.9912 - val_loss: 1.3766 - val_acc: 0.6210\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 0.0302 - acc: 0.9906 - val_loss: 1.2318 - val_acc: 0.6650\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 0.0277 - acc: 0.9938 - val_loss: 1.2223 - val_acc: 0.7010\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0305 - acc: 0.9900 - val_loss: 1.1976 - val_acc: 0.6790\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 6s 121ms/step - loss: 0.0257 - acc: 0.9931 - val_loss: 1.1722 - val_acc: 0.6560\n"
     ]
    }
   ],
   "source": [
    "modelid = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=50),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='model_checkpoint_best_{}.h5'.format(modelid),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True),\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='./logs/'+modelid,\n",
    "        histogram_freq=0, write_graph=False, write_images=False)\n",
    "]\n",
    "\n",
    "model = makeModel(32)\n",
    "history=model.fit_generator(\n",
    "                    generator(dataset_tr, batch_size, patchsize),\n",
    "                    steps_per_epoch=50, \n",
    "                    epochs=50, \n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test,y_test),\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After training our model, we can save it to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"input32\"\n",
    "\n",
    "# Save model to a file\n",
    "keras.models.save_model(model,\"{}.model\".format(modelname))\n",
    "\n",
    "# Also save the testing dataset (may be large) so we can pick up from here later \n",
    "dataset_te.to_pickle(\"{}.testingdata.pickle\".format(modelname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you have time and want to experiment, you may train many networks exploring the parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train many models\n",
    "for filters in [1,2,4,8,16,32,48,64,96]:\n",
    "    modelid = \"filters{:03d}_timestamp{}\".format(filters,time.strftime(\"%Y%m%d%H%M%S\"))\n",
    "\n",
    "    callbacks_list = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_acc',\n",
    "            patience=50),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath='model_checkpoint_best_{}.h5'.format(modelid),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True),\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir='./logs/'+modelid,\n",
    "            histogram_freq=0, write_graph=False, write_images=False)\n",
    "    ]\n",
    "    \n",
    "    model = makeModel(filters)\n",
    "    print(model.summary())\n",
    "    print(model.count_params())\n",
    "\n",
    "    history=model.fit_generator(\n",
    "                        generator(dataset_tr, batch_size, patchsize),\n",
    "                        steps_per_epoch=50, \n",
    "                        epochs=1, \n",
    "                        verbose=1,\n",
    "                        validation_data=(X_test,y_test),\n",
    "                        callbacks=callbacks_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 6: Process the images of the testing set one by one\n",
    "And visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, load a saved model and testing dataset\n",
    "modelname = \"models/model_venus\"\n",
    "\n",
    "model = keras.models.load_model(\"{}.model\".format(modelname))\n",
    "patchsize = model.input.shape[1].value\n",
    "dataset_te = pd.read_pickle(\"{}.testingdata.pickle\".format(modelname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show results by processing a single variation of the testing image\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "def resultsShow(i, data, model):\n",
    "    d = data.iloc[i]\n",
    "    im = d[\"image\"]\n",
    "    l = d[\"label\"]\n",
    "    fig,axs = plt.subplots(nrows=1,ncols=3,figsize=(15,5),gridspec_kw={'width_ratios':[1,1,0.5]})\n",
    "    \n",
    "    imt = transform_simple(im, patchsize)\n",
    "    axs[0].imshow(im)\n",
    "    axs[0].set_title(\"Image (true class: {})\".format(names[l]))\n",
    "    \n",
    "    axs[1].imshow(imt,interpolation=\"nearest\")\n",
    "    axs[1].set_title(\"Network input\")\n",
    "    \n",
    "    outs = model.predict(np.array([imt]))\n",
    "    print(outs)\n",
    "    predicted = np.argmax(outs)\n",
    "    axs[2].bar(np.array(range(len(names))), outs[0,:], 1, color=\"gray\")\n",
    "    axs[2].set_ylim([0,1])\n",
    "    axs[2].set_xticks(range(len(names)))\n",
    "    axs[2].set_xticklabels(names)\n",
    "    axs[2].set_ylabel(\"probability\")\n",
    "    axs[2].set_xlabel(\"class\")\n",
    "    axs[2].set_title(\"Network output\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(\"out_{:05d}_{}.png\".format(i,(\"ok\" if predicted==l else \"ko\")))\n",
    "    print(outs)\n",
    "\n",
    "interact(resultsShow, i=widgets.IntSlider(min=0,max=len(dataset_te)-1, step=1, value=0, continuous_update=False), data=fixed(dataset_te.sample(len(dataset_te))), model=fixed(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show results by processing a single variation of the testing image\n",
    "import warnings\n",
    "\n",
    "dataset_te_new = dataset.loc[dataset.dn == \"new\"]\n",
    "tdata = dataset_te_new.iloc[1:,:]\n",
    "\n",
    "%matplotlib inline\n",
    "def resultsShow(i, data, model):\n",
    "    d = data.iloc[i]\n",
    "    im = d[\"image\"]\n",
    "    l = d[\"label\"]\n",
    "    fig,axs = plt.subplots(nrows=1,ncols=3,figsize=(10,4),\n",
    "                           gridspec_kw={'width_ratios':[1,1,0.5]}, dpi=200)\n",
    "    \n",
    "    imt = transform_simple(im, patchsize)\n",
    "    axs[0].imshow(im)\n",
    "    axs[0].set_title(\"Image (true class: {})\".format(names[l]))\n",
    "    \n",
    "    axs[1].imshow(imt,interpolation=\"nearest\")\n",
    "    axs[1].set_title(\"Network input\")\n",
    "    \n",
    "    for ax in axs[:2]:\n",
    "        ax.set(xticks=[], yticks=[])\n",
    "    \n",
    "    outs = model.predict(np.array([transform_complex(im, patchsize) for _ in range(100)]))\n",
    "    outs = np.mean(outs, axis=0)[np.newaxis,:]\n",
    "    predicted = np.argmax(outs)\n",
    "    axs[2].bar(np.array(range(len(names))), outs[0,:], 1, color=\"gray\")\n",
    "    axs[2].set_ylim([0,1])\n",
    "    axs[2].set_xticks(range(len(names)))\n",
    "    axs[2].set_xticklabels(names)\n",
    "    axs[2].set_ylabel(\"probability\")\n",
    "    axs[2].set_xlabel(\"class\")\n",
    "    axs[2].set_title(\"Network output\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"out_new_{:05d}.png\".format(i))\n",
    "    plt.show()\n",
    "    \n",
    "interact(resultsShow, i=widgets.IntSlider(min=0,max=len(tdata)-1, step=1, value=0, continuous_update=False), data=fixed(tdata), model=fixed(model));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Save processed variations\n",
    "import warnings\n",
    "import itertools\n",
    "outdir = pathlib.Path(\"out\")\n",
    "if(not outdir.exists()):\n",
    "    outdir.mkdir()\n",
    "for i,j in itertools.product(range(len(dataset_te_new)),range(20)):\n",
    "    d = dataset_te_new.iloc[i,:]\n",
    "    im = d[\"image\"]\n",
    "    l = d[\"label\"]\n",
    "    fig,axs = plt.subplots(nrows=1,ncols=3,figsize=(10,4),\n",
    "                           gridspec_kw={'width_ratios':[1,1,0.5]}, dpi=200)\n",
    "    imt = transform_complex(im, patchsize)\n",
    "    axs[0].imshow(im)\n",
    "    axs[0].set_title(\"Image (true class: {})\".format(names[l]))\n",
    "\n",
    "    axs[1].imshow(imt,interpolation=\"nearest\")\n",
    "    axs[1].set_title(\"Network input\")\n",
    "\n",
    "    for ax in axs[:2]:\n",
    "        ax.set(xticks=[], yticks=[])\n",
    "\n",
    "    outs = model.predict(np.array([imt]))\n",
    "    predicted = np.argmax(outs)\n",
    "    axs[2].bar(np.array(range(len(names))), outs[0,:], 1, color=\"gray\")\n",
    "    axs[2].set_ylim([0,1])\n",
    "    axs[2].set_xticks(range(len(names)))\n",
    "    axs[2].set_xticklabels(names)\n",
    "    axs[2].set_ylabel(\"probability\")\n",
    "    axs[2].set_xlabel(\"class\")\n",
    "    axs[2].set_title(\"Network output\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(str(outdir/\"out_new_{:05d}_aug_{:05d}.png\".format(i,j)))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualize the filters learned by the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].get_weights()[0][:,:,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize filters in a layer\n",
    "import itertools\n",
    "filters = model.layers[2].get_weights()[0]\n",
    "filters.shape\n",
    "fig, axs = plt.subplots(nrows = filters.shape[2], ncols = filters.shape[3], figsize=(20,10))\n",
    "for i,j in itertools.product(range(filters.shape[2]),range(filters.shape[3])):\n",
    "    axs[i,j].imshow(filters[:,:,i,j],vmin=-0.5,vmax=+0.5,cmap=\"gray\")\n",
    "    axs[i,j].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualize the activations in the intermediate layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def plot_hidden_layers(imt,nmaps=4):\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    plt.title('Hidden layers', loc='center')\n",
    "    plt.axis('off')\n",
    "    layers = [model.layers[0].input] + [model.layers[i].output for i in [0,2,5,7,9]]\n",
    "    for layeri,layer in enumerate(layers):\n",
    "        get = K.function([model.layers[0].input], [layer])\n",
    "        layeroutputs = get([imt[np.newaxis,:,:,:]])[0][0]\n",
    "        for j in range(nmaps): # for each map\n",
    "            sp2=fig.add_subplot(nmaps,len(layers),layeri+1+j*len(layers))\n",
    "            sp2.axis('off')\n",
    "            if (layeroutputs.shape[2]>j):\n",
    "                vmin,vmax = ((0,1) if layeri == 0 else (-0.3,+0.3))\n",
    "                sp2.imshow(layeroutputs[:,:,j],\n",
    "                           cmap=\"gray\",\n",
    "                           interpolation=\"nearest\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "imt=transform(dataset_te[\"image\"].iloc[0],patchsize)\n",
    "plot_hidden_layers(imt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualization 2: Input that maximizes a neuron (or map)\n",
    "We now use gradient descent to find an input image maximizing a neuron or internal map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def makeinputmaximizing(loss, gradstep = 0.01, steps = 20):\n",
    "    grads = K.gradients(loss, model.input)[0]\n",
    "    grads /= K.sqrt(K.mean(K.square(grads)) + 1e-5)\n",
    "    iterate = K.function([model.input, K.learning_phase()],[loss, grads])\n",
    "\n",
    "    # img = transform_simple(dataset_te[\"image\"].iloc[0],patchsize)[np.newaxis,:,:,:]\n",
    "    img = 0.5+0.01*np.random.rand(1,patchsize,patchsize,3)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        loss_value, grads_value = iterate([img, 1])     \n",
    "        img += grads_value * gradstep\n",
    "        img = np.clip(img,0,1)\n",
    "    return img[0,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(makeinputmaximizing(model.layers[6].output[0,2], steps = 1000, gradstep = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network\n",
    "model_name=\"rps.model\"\n",
    "model = keras.models.load_model(model_name)\n",
    "patchsize=64\n",
    "\n",
    "def makeinputmaximizing(loss, gradstep = 0.01, steps = 20):\n",
    "    grads = K.gradients(loss, model.input)[0]\n",
    "#    grads /= K.sqrt(K.mean(K.square(grads)) + 1e-5)\n",
    "    iterate = K.function([model.input, K.learning_phase()],[loss, grads])\n",
    "\n",
    "    # img = transform_simple(dataset_te[\"image\"].iloc[0],patchsize)[np.newaxis,:,:,:]\n",
    "    img = 0.5+0.01*np.random.rand(1,patchsize,patchsize,3)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        loss_value, grads_value = iterate([img, 1])\n",
    "        if np.max(grads_value)>0:\n",
    "            gradstep=1/np.max(grads_value)\n",
    "        img += grads_value * gradstep\n",
    "        img = np.clip(img,0,1)\n",
    "    return img[0,:,:,:]\n",
    "\n",
    "plt.imshow(makeinputmaximizing(model.layers[4].output[0,8,2], steps = 100, gradstep = 0.1))\n",
    "plt.show()\n",
    "\n",
    "import tqdm\n",
    "import itertools\n",
    "out = model.layers[6].output\n",
    "fig, axs = plt.subplots(nrows = 2, ncols = 5, figsize=(20,5), squeeze = False)\n",
    "for i,j in tqdm.tqdm(list(itertools.product(range(axs.shape[0]), range(axs.shape[1])))):\n",
    "    print (i)\n",
    "    print (j)\n",
    "    loss = K.mean(out[0,j])\n",
    "    img = makeinputmaximizing(loss, gradstep = 0.03, steps = 20)\n",
    "    \n",
    "    axs[i,j].imshow(img)\n",
    "    axs[i,j].axis(\"off\")\n",
    "    plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualization 3: Let us look for an 'ideal' rock/paper/scissors"
    "## Let us look for what the convnet considers an ideal instance of a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = [makeinputmaximizing(model.output[0,k], gradstep=0.02, steps=50) for k in range(3)]\n",
    "fig,ax = plt.subplots(figsize=(15,5))\n",
    "ax.imshow(np.hstack(final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualization 4: Heatmap\n",
    "Let us try to highlight the portions of the input image that most influence the classification decision. This technique also allows one to locate an object in an image.\n",
    "- First, we feed an image of a paper gesture to the network;\n",
    "- Second, we assign to each channel in the output of the last convolution a value corresponding ot its contribution to the class \"paper\";\n",
    "- Third, we consider which parts in the input image activate the channels that lead to the decision: this is a paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import skimage.viewer\n",
    "import cv2\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "\n",
    "# load the network\n",
    "model_name=\"rps.model\"\n",
    "model = keras.models.load_model(model_name)\n",
    "\n",
    "# load an input image\n",
    "img=skimage.img_as_float(skimage.io.imread(\"paper.jpg\"))\n",
    "img=np.expand_dims(img, axis=0)\n",
    "\n",
    "# predict its class\n",
    "preds=model.predict(img)\n",
    "output=model.output[:, np.argmax(preds[0])]\n",
    "\n",
    "# find the last convolutional layer\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, layers.Conv2D):\n",
    "        layer_name=layer.name\n",
    "        layer_filters=layer.filters\n",
    "last_conv_layer=model.get_layer(layer_name)\n",
    "\n",
    "# compute the gradient of the output wrt to last convolutional layer\n",
    "gradient=K.mean(K.gradients(output, last_conv_layer.output)[0], axis=(0, 1, 2))\n",
    "function=K.function([model.input, K.learning_phase()], [gradient, last_conv_layer.output[0]])\n",
    "map_weights, pixel_values = function([img, 1])\n",
    "\n",
    "# multiply it by the pixel values of the last convolutional layer\n",
    "for i in range(layer_filters):\n",
    "    pixel_values[:,:,i] *= map_weights[i]\n",
    "\n",
    "# postprocess the heatmap\n",
    "heatmap = np.mean(pixel_values[:,:,:], axis=-1)\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.max(heatmap)\n",
    "    \n",
    "# visualize it\n",
    "img=cv2.imread('paper.jpg')\n",
    "heatmap=cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "heatmap=np.uint8(255 * heatmap)\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "plt.imshow(heatmap)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "livereveal": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
